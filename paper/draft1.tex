\documentclass[letterpaper,12pt]{article}

\usepackage[margin=1.5in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{geometry}
\usepackage{physics}
\usepackage{hyperref}
\lstset{
  tabsize=2
}


\renewenvironment{abstract}
 {
  \begin{center}
  \vspace{3em} \bfseries \abstractname\vspace{0em}\vspace{0pt}
  \end{center}
  \list{}{%
    \setlength{\leftmargin}{20mm}% <---------- CHANGE HERE
    \setlength{\rightmargin}{\leftmargin}%
  }%
  \item\relax}
 {\endlist}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Var}{Var}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathbbm{1}}
\newcommand*{\QED}{\hfill\ensuremath{\square}}%

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}


\begin{document}

% \title{ELF}
% \author{Anish Thilagar}
% \date{\today}
% \maketitle


% \begin{abstract}
 
% \end{abstract}
\section{Name???}
- SHELF\\
- GNOME\\
- ???


keywords: event, forecast, voting

EFV

\section{Model and Definitions}
Our model will consist of $n$ forecasters s.t. $n \ge 2$ where $i \in [1,n]$ denotes the $i$th forecaster, $m$ independent events where $k \in [1,m]$ denotes the $k$th event. Each event $k \in [1,m]$ will be associated with a random variable $X_k$. $X_k = 1$ will denote that event $k$ occurred, and, likewise, $X_k = 0$ will denote event $k$ did not occur. $\forall k \in [m] \exists \theta_k$ s.t. $\theta_k$ denotes the true, unknown probability of event $k$ occurring;  $\theta_k = P(X_k = 1) = 1 - P(X_k = 0)$. We will denote each outcome drawn as $x_i \sim X_i$. For each forecaster $i$, we denote their true belief that event $k$ will occur as $\hat{\theta}_{i,k} \in (0,1)$. We then denote $y_{i,k} \in [0,1]$ as the actual report forecaster $i$ makes for event $k$. Note that $y_{i,k}$ need not equal $\hat{\theta}_{i,k}$. \\

After every event $k \in [1,m]$ has occurred, and thus each forecaster has reported $y_{i,k} \forall i f\forall k$, a mechanism will select exactly one forecaster from the set of $[n]$ as the 'best forecaster'.\\

\textbf{Definition 1: Forecaster Selection Mechanism:}\\
Some mechanism $M(y, x)$ that maps the forecasts $y = (y_1, ..., y_n)$ and outcomes of the events $x = (x_1, ..., x_k)$ to an index $w \in [1, n]$. We say that forecaster $w$ wins the competition if $M(y, x) = w$.\\

We call a forecaster selection mechanism truthful iff, for all forecasters, their probability of winning is maximized when they reports their beliefs as their forecasts.\\

\textbf{Definition 2: Truthful:}\\
$M$ is truthful iff $\forall i \in [n]$, $\forall \hat{\theta}_i$, $\forall y^{\prime}_i \neq \hat{\theta}_i$, and $\forall y_j s.t. j \neq i$,
\[ P(M( (y_1, ..., \hat \theta_i, .., y_n), x ) = i) \geq  P(M( (y_1, ..., \hat y_i, .., y_n), x ) = i)\]
\textbf{Note:} $M$ is strictly truthful if the strict inequality inequality holds.\\

Additionally, we would like our mechanism to choose the forecaster with the best predictions. We define the accuracy of a single forecaster $i$ as 1 minus the average squared loss of their forecasts compared to the true probabilities.\\

\textbf{Definition 3: Accuracy:}\\
$a_i = 1 - \frac{1}{m}\sum_{k=1}^m (y_{i, k} - \theta_k)^2$.\\

An ideal mechanism would choose forecasters with larger accuracies with higher probabilities. Let $i^* = \argmax_i a_i$ be the highest accuracy forecaster, and let $a^* = a_{i^*}$.\\

\textbf{Definition 4: Accuracy Gap:}\\
$\epsilon = a^* - \max_{i \neq i^*} a_i$ denote the accuracy gap.\\

For a given mechanism $M$ and $n$ forecasters with accuracy gap $\epsilon$, we let $m_\delta$ be the minimum number of events such that $M$ necessarily chooses $i^*$ with probability at least $1 - \delta$. Specifically, for any $m > m_\delta(M, n, \epsilon)$, $P(M(y, x) \neq i^*) \leq \delta$. $m_\delta$ gives us a measure of how many events are needed for a mechanism to stably pick the best forecaster. We will use it as a metric to compare mechanisms, where we say $M$ is more efficient than $M'$ if $m_\delta(M, n, \epsilon) < m_\delta(M', n, \epsilon)$, since $M$ needs less events to consistently pick the best forecaster.\\

Similar to work in [ELF Citation], we will focus on using the \textbf{proper scoring rule}, the \textbf{quadratic scoring rule} in our forecaster selection mechanism.\\

\textbf{Definition 5: Proper Scoring Rule}:\\
Scoring rule, $R$ is proper if $\forall p,y \in {0,1}, E_{x~p}[R(p,x)] \ge E_{x~p}[R(y,x)]$\\

\textbf{Definition 6: Quadratic Scoring Rule}:\\
$R_q = 1 - (y - x)^2$; Where $x$ denotes the observed outcome, and $y$ denotes the report.

\section{Simple average mechanism}
A simple selection mechanism that is often done in practice, we will call the simple average mechanism $M_s$. For this mechanism, we assign each forecaster a score $f_{i, k} = R_q(y_{i, k}, x_k)$ for each event. Then, we assign their final score by summing these over all events, $F_i = \sum_{j=1}^m f_{i, j}$. Finally, we choose the forecaster with the highest cumulative score $M_s(y, x) = \argmax_i F_i$. The idea of this mechanism is that each forecaster's final score in this mechanism is $F_i = \sum_{i=1}^m 1 - (y_{i, j} - x_i)^2$ is an analog for their cumulative accuracy, $m a_i = \sum_{k=1}^m 1 - (y_{i, k} - \theta_k)^2$. Therefore, the forecaster with the highest score will correlate to the forecaster with the highest accuracy. However, we will show that this is not necessarily the case. 

\subsection{Truthfulness}
show simple counterexample(s?) \\
- single event favors extremes \\
- multiple events can be dominated by single untruthful event when rest are isomorphic \\

\subsection{Accuracy}
Even though $M_s$ is not truthful, we assume that forecasters still report their true beliefs $\hat \theta_i$. For a single event, the most extreme forecasters will always get selected. For example, if the single outcome is $x = 0$, the forecaster with the minimum $\hat \theta_i $ will have the highest score, so they will always win. Similarly if $x = 1$, the highest belief forecaster will always win. 

However, for multiple events, the averaging starts to favor more forecasters. We will see that we get a similar $m_\delta$ as the SHELF mechanism introduced later, assuming the forecasters remain truthful. In fact, the main advantage of SHELF is that it is as efficient as $M_s$ while also being truthful.

\section{ELF}
\newcommand{\elf}{M_{\mathrm{ELF}}}
The ELF mechanism $\elf$ \cite{elf}, improves upon $M_s$, by incentivizing forecasters to be truthful. For each event $k$, we hold a lottery to choose a winner. Forecaster $i$ is chosen with probability
\[ f_{i, k} = \frac{1}{n} + \frac{1}{n} \left(R_q(y_{i, k}, x_k) - \frac{\sum_{j\neq i} R_q(y_{j, k}, x_k)}{n-1} \right)\]
This score is the additively normalized quadratic score among all forecasters. A forecaster increasing their own quadratic score increases the chances they win an event, while decreasing the chances others win that event. Let $w_k$ be the winner of event $k$. Then we choose the forecaster $\elf(y, x) = \argmax_i \sum_{k=1}^m \1(w_k = i)$ as the winner of the competition. Essentially, we give the winner of each event lottery a point, and then pick the forecaster who got the most points as the overall winner.

In Theorem 6 of \cite{elf}, it is proven that $\elf$ is strictly truthful. In Theorem 8 of the same paper, they also upper bound $m_\delta$ by showing 
\[ m_\delta(\elf, n, \epsilon) \leq \frac{2(n-1)^2}{\epsilon^2} \ln \left(\frac{4(n-1)}{\delta}\right)\]
For a fixed forecaster accuracy gap and desired mechanism accuracy, the number of events ELF needs to consistently choose the right forecaster is proportional to $\frac{1}{\epsilon^2} n^2 \ln n$. However, this upper bound is not tight. 

Let $F_{i, k}$ be the indicator function for forecaster $i$ winning the lottery for event $k$. Then, $F_i = \sum_k F_{i, k}$ is the random variable that is the number of events forecaster $i$ wins. The proof of Theorem 8 \cite{elf} utilized the Hoeffding bound applied to $F_i$, which assumes they have high variance. However, this is not the case, and utilizing that fact gives an improvement in the lower bound. 
\begin{theorem}[Bernstein's Inequality]
  \label{bernstein}
  Given random variables $X_i$ for $1 \leq i \leq n$ such that $0 \leq X_i \leq 1$ almost surely, let $S = \sum_i X_i$. Then,
  \[ P\left(|S - \E[S]| < t\right) < 2 \: e^{\frac{-t^2 }{2\left(\Var(S) + \frac{t}{3}\right)} } \]
\end{theorem}
\begin{theorem}
  \label{elf_bound}
    $m_\delta(\elf, n, \epsilon) \leq \frac{5(n-1)}{\epsilon^2}\ln\left(\frac{4(n-1)}{\delta}\right)$
    % For an accuracy gap $\epsilon$, ELF correctly chooses the most accurate forecaster with probability at least $1 - 4 (n-1) e^{-\frac{m \epsilon}{6(n - 1)}}$.
\end{theorem}
\emph{Proof:} Note that for any single event $k$, every forecaster wins with probability at best $\frac{2}{n}$, and at worst $\frac{1}{n} - \frac{1}{n(n-1)}$ (achieved when $y_i = 1$, $y_{j\neq i} = 0$, and $x_i = 1$). Since $F_{i, k}$ is just a binary random variable, it will have highest variance when it's expectation is as close to $\frac{1}{2}$ as possible. For nontrivially small $n$, this will mean its expectation is $\frac{2}{n}$ (or equivalently $P(F_{i, k} = 1) = \frac{2}{n}$), so its variance will be at most $\frac{2(n-2)}{n^2}$. So we have $\Var(F_{i, k}) \leq \frac{2(n-2)}{n^2} < \frac{2}{n}$. 

Since $F_i = \sum_k F_{i, k}$, we have $\Var(F_i) = \sum_k \Var(F_{i, k}) < \sum_k \frac{2}{n} = \frac{2m}{n}$. As shown in [elf paper], for $j\neq i$ we have 
\[ \E[F_i] - \E[F_j] \geq \frac{m \epsilon}{n - 1}\]
Therefore, if $F_j \geq F_i$, (forecaster $j$ beats forecaster $i$), then either forecaster $j$ overperformed their expectation by at least $\frac{m \epsilon}{2(n - 1)}$, or forecaster $i$ underperformed their expectation by $\frac{m \epsilon}{n - 1}$. Specifically, either $\E[F_i] - F_i \geq \frac{m \epsilon}{2(n - 1)}$ or $F_j - \E[F_j] \geq \frac{m \epsilon}{2(n - 1)}$

Applying Theorem \ref{bernstein}, we have
\begin{align*}
    P\left(\left| F_i - \E[F_i]\right| < \frac{m \epsilon}{2(n - 1)} \right) 
    &< 2 \: e^{\frac{-\left(\frac{m \epsilon}{2(n - 1)}\right)^2 }{2\left(\frac{2m}{n} + \frac{m \epsilon}{6(n - 1)}\right)}} \\
    &< 2 \: e^{\frac{-\left(\frac{m \epsilon}{2(n - 1)}\right)^2 }{2\left(\frac{m}{n-1}(2 + \frac{\epsilon}{6}) \right)}} \\
    &= 2 \: e^{\frac{-\frac{m \epsilon^2}{2(n - 1)} }{(2 + \frac{\epsilon}{6})}} \\
    &< 2 \: e^{-\frac{m \epsilon^2}{5(n - 1)} }  \numberthis \label{elf_bernstein} 
\end{align*}
And similarly for $F_j$. Putting both of those cases together, we have by the union bound
\begin{align*}
  P(F_j \geq F_i) &= P\left(\left(F_j - \E[F_j] < \frac{m \epsilon^2}{2(n - 1)}\right) \cup \left(\E[F_i] - F_i < \frac{m \epsilon^2}{2(n - 1)} \right) \right) \\
  &\leq P\left(F_j - \E[F_j] < \frac{m \epsilon^2}{2(n - 1)}\right) + P\left(\E[F_i] - F_i < \frac{m \epsilon^2}{2(n - 1)} \right) \\
  &\leq 4 \: e^{-\frac{m \epsilon^2}{5(n - 1)} }
\end{align*}
Using the union bound over all $j$, we have
\begin{align*}
  P\left(M_l(y_1, ..., y_n, x) = i\right)
  &= 1 - \sum_{j\neq i} P\left(M_l(y_1, ..., y_n, x) = j\right) \\
  &\geq 1 - \sum_{j\neq i} P\left(F_j \geq F_i\right) \\
  &\geq 1 - 4 (n-1) e^{-\frac{m \epsilon^2}{5(n - 1)}} 
\end{align*}

Assigning this probability to $1 - \delta$, we see that for a fixed $n, \epsilon$, ELF will choose the correct forecaster with probability at least $1 - \delta$ if $m$ is large enough for the above inequality to hold. Solving it for $m$, we get the minimum such satisfying value
\[m_\delta \leq \frac{5(n-1)}{\epsilon^2}\ln\left(\frac{4(n-1)}{\delta}\right) \]
\hfill\QED

\section{SHELF}
\newcommand{\shelf}{M_{\mathrm{SHELF}}}

For this mechanism, we let the binary random variable $X_{i, k}$ be 1 with probability $R_q(y_{i, k}, x_i)$, and 0 otherwise. Then, for each event $k$ we assign forecaster $i$ the score $f_{i, k} = X_{i, k}$ obtained by sampling this variable. Finally, we assign their final score by summing these over all events, $F_i = \sum_{j=1}^m f_{i, j}$, then chose the forecaster with the highest score $\shelf(y, x) = \argmax_i F_i$, breaking ties uniformly at random. 

\subsection{Truthfulness}
Note: The following analysis is inspired by and follows the work done in [ELF Citation].\\\\
For a single event with true probability $\theta$, every forecaster will either win 1 or 0 events. The probability that their final score is 1 will just be their expected score for the single event. Therefore, to maximize their final score, each forecaster must maximize their expected score for the single event. Their score is just $\E_{x \sim \theta}(R_q(y_i, x)) = \E_{x \sim \theta}(1 - (y_i - x)^2)$. This is maximized when $y_i = \theta$, so the mechanism is truthful. (what theorem 4 does) We can also see this directly be noting that $R_q$ is a strictly proper scoring rule, so its expectation is maximized when $y_i = \theta$. 

\begin{theorem}
  \label{shelf_truthful}
    $\shelf$ is truthful for $m \geq 1$ events.
\end{theorem}
\emph{Proof:} For each forecaster $j \in [1, n]$ fix forcaster's report.

S'pose that for some forecaster $i$, $i$'s report on some subset of events, where $k \in [1, m]$ denotes the size of this subset, does not maximize their expected score for any of those individual events.\\
Let $k^\prime$ be one of those events.

We will now show that $i$ will maximize their probability of being chosen as the best forecaster in the SHELF mechanism iff they alter their $k$ reports to maximize their expected score on each $k$ events. A guiding intuition here would be as follows: The higher the expected score a forecaster has on a given event, the higher the probability that SHELF awards them a point on this specific event, and the only way to maximize one's probability of being selected as the best forecaster is to increase the number of points they are awarded. Therefore, for a single independent event, increasing one's expected score will either increase the probability of being selected as the best forecaster or leave it unchanged, and it does so without altering results of any other event. In this way, we will show that being truthful for a singular event will always increase one's probability of being selected as the best forecaster. We can then repeat this process for each untruthful report to maximize the probability of being selected as the best forecaster.\\\\
Formally, we will analyze three cases relating to the context provided by the $m - 1$ events other than $k^\prime$.

Let $V(i)$ denote the number of points forecaster $i$ has accrued.\\
Let $f_{i,k}$ denote the probability forecaster $i$ scores a point on event $k$.\\

\begin{enumerate}
    \item Some forecaster $j$ scores at least two more points than $i$, OR $i$ scores at least two more points than any other forecaster.\\
    $$\exists j \in [1,m] \text{ s.t. } j \ne i \text{ and } V(j) - V(i) \geq 2$$ 
    $$OR$$
    $$\forall j \in [1,m] \text{ s.t. } j \ne i; V(i) - V(j) \geq 2$$
    For this case, $i$'s probability of being selected as the best forecaster is either $1$ or $0$. It is also unaffected by being truthful or not in $k^\prime$, because gaining, or failing to gain, $1$ point in this context is not enough to alter the outcome as only the forecaster with the most number of points is chosen as the best forecaster with ties broken uniformly.
    
    \item Some forecaster $j$ scores exactly one more point than $i$, AND no other forecasters score strictly more points than $i$.\\
    $$\exists j \in [1,m] \text{ s.t. } j \neq i \text{ and } V(j) - V(i) = 1$$
    $$AND$$
    $$\forall l \in [1,m] \text{ s.t. } l \neq i \text{ and } l \neq j; V(i) - V(l) \geq 0$$
    For this case, $i$ must seek to maximize $f_{i,k^\prime}$ to have any non-zero probability of being selected as the best forecaster under the SHELF mechanism. 
    In the SHELF mechanism, the maximum payout a forecaster can receive for a single event is $1$, and that point is awarded to them with probability exactly equal to their quadratic score. Thus, we know a forecaster's expected value for a singular event is exactly their quadratic score. Since the quadratic scoring rule is proper, we know that the expectation of a forecaster's quadratic score is maximized when the forecaster is reporting their true beliefs. As a result, we can conclude that the only way for $i$ to maximize their probability of receiving a point in $k^\prime$ is to report their true beliefs. By reporting their true beliefs, which maximizes the probability that they score another point, $i$ also maximizes the probability of being selected as the best forecaster which, in this context, occurs when only $i$ scores a point and the tie between $j$ and $i$ is broken uniformly via the rules of SHELF.
    
    \item No forecasters score more points than $i$, but there is at least one forecaster $j$ that scores the same number of points as $i$ or scores exactly one less point than $y_i$.
    $$\exists j \in [1,m] \text{ s.t. } j \ne i \text{ and } V(i) - V(j) \in \{0,1\}$$
    In any given configuration, a forecaster's probability of being selected as the best forecaster is maximized when the minimal number of other forecaster have as many points as them. In the SHELF mechanism, any forecaster's probability of scoring a point on a given event is independent of all other. Therefore, there is no action that can be taken to minimize the probability of any competitor's probability of being selected as the best forecaster except strictly increasing one's own point total. In this final case, the only way for $i$ to increase their probability of being chosen as the best forecaster would be to increase the probability of scoring a point in $k^\prime$. The same logic as case 2 holds here that $i$ would then prefer to be truthful and maximize their expected score in $k^\prime$ to increase their probability of being selected as the best forecaster.
\end{enumerate}

Since any configuration of the $m-1$ events falls into one of these three cases, we have shown that any forecaster is weakly incentivized to report truthfully.
To show that any forecaster is strongly incentivized to report truthfully, we simply show that $i$ believes cases 2 or 3 will happen with positive probability. Since $y_{j,k} = p_{j,k} \in (0,1) \forall j \forall k$, we know that every forecaster has some uncertainty on each event. As a result, we can conclude that $f_{j,k} > 0 \forall j \forall k$. We can think of all possible outcomes as the set of all possible binary strings of length $n$, and since $f_{j,k} > 0 \forall j \forall k$ there exists at least one sequence of outcomes strings picked from our set that constitutes cases 2 or 3. As a result, there is a positive probability that cases 2 or 3 occur, and thus forecasters are strongly incentivized to report truthfully.  

\subsection{Accuracy}

% From Proposition 2 of \cite{elf}, we have that $E_{x \sim \theta}(R_q(y, x)) = R_q(y, \theta) + \theta^2 - \theta$. Additionally, for a single event, a forecaster's accuracy $a_i$ is by definition just $R_q(y, \theta)$. So, $E_{x \sim \theta}(R_q(y, x)) = a_i + \theta^2 - \theta$. For a single event with probability $\theta$, forecaster $i$'s expected score is $\E_{x \sim \theta} f_i = E_{x \sim \theta}(R_q(y_i, x)) = a_i + \theta^2 - \theta$.

% \subsection{Truthfulness for a single event}
% (section 4.3, proposition 5).  For forecaster $i$ to be selected, they either need to have $f_i = 1$, and then be chosen uniformly at random from all others who scored 1, or everyone must score 0 and they will be chosen uniformly at random. In the first case, the expected number of other forecasters who also score 1 is $\sum_{i' \neq i} \E_{x \sim \theta} f_{i'} = \sum_{i' \neq i} (a_{i'} + \theta^2 - \theta) = (n-1) (\theta^2 - \theta) \sum_{i' \neq i} a_{i'}$. Then, the probability that forecaster $i$ is chosen is 
% \[
%   P(f_i = 1) \frac{1}{1 + (n-1) (\theta^2 - \theta) \sum_{i' \neq i} a_{i'}} = \frac{a_i +\theta^2 - \theta}{1 + (n-1) (\theta^2 - \theta) \sum_{i' \neq i} a_{i'}}
% \]
% In the other case, the probability $i$ wins is just 
% \[
%   \frac{1}{n} \prod_{i} P(f_i = 0) = \frac{\prod_i (1 - a_i -\theta^2 + \theta)}{n}
% \]

% There's probably a better way to do this... Actually coming back to this I'm not even sure that the first step is allowed (using expected number of winners directly instead of summing over cases)

% \subsection{Truthfulness for multiple events}
\begin{theorem}
  \label{shelf_bound}
    $m_\delta(\shelf, n, \epsilon) \leq \frac{2}{\epsilon^2} \ln \frac{4(n-1)}{\delta}$
\end{theorem}
\emph{Proof:} This proof is a slight modification of the proof of Theorem 8 in \cite{elf}.

We first bound the difference in the expected number of wins of $i^*$ and $j$, for any $j \neq i^*$. 
\begin{align*}
  \E \left( \sum_{k=1}^m f_{i^*, k}\right) - \E \left( \sum_{k=1}^m f_{j, k}\right)
  &= \left( \sum_{k=1}^m \E f_{i^*, k}\right) - \left( \sum_{k=1}^m \E f_{j, k}\right)\\
  &= \sum_{k=1}^m \left(\E (f_{i^*, k}) - \E (f_{j, k})\right)\\
  &= \sum_{k=1}^m \left((a_{i^*} + \theta_k^2 - \theta_k) - (a_j + \theta_k^2 - \theta_k)\right)\\
  &= \sum_{k=1}^m \left(a_{i^*} - a_j\right)\\
  &\geq \sum_{k=1}^m \epsilon\\
  &=m \epsilon
\end{align*}
If $F_j \geq F_{i^*}$, then either $\E(F_{i^*}) - F_{i^*} \geq \frac{m \epsilon}{2}$ or $F_j - \E(F_j) \geq \frac{m \epsilon}{2}$. By Hoeffding's, we can upper bound the probability of each of these events by $2 e^{-\frac{2 (\frac{m \epsilon}{2})^2}{m}} = 2 e^{\frac{- m \epsilon^2}{2}}$. Using the union bound, the probability that either of those events occurs is twice this, or $4 e^{\frac{- m \epsilon^2}{2}}$.

Using the union bound again over all $j \neq i$, we have 
\begin{align*}
  P(\shelf(y, x) = i) &= P(F_{i^*} > F_j \quad \forall j \neq i^*) \\
                      &\geq 1 - \sum_{j\neq i^*} P(F_{i^*} \leq F_j) \\
                      &= 1 - 4 (n-1) e^{\frac{- m \epsilon^2}{2}}
\end{align*}

Letting this quantity be $1 - \delta$ and solving for $m$, we see that this mechanism correctly chooses forecaster $i$ with probability $1 - \delta$ when $m$ is large enough to satisfy the inequality. Therefore
\[ m_\delta(\shelf, n, \epsilon) \leq \frac{2}{\epsilon^2} \ln \frac{4(n-1)}{\delta} \]
\hfill \QED

Comparing this to the result of Theorem \ref{elf_bound}, we see that $\shelf$ is much more efficient than $\elf$. We can also directly apply the proof of Theorem \ref{shelf_bound} to $M_s$, since the mean and variance of $F_i$ is the same in both cases. Therefore, $m_\delta(\shelf, n, \epsilon) = m_\delta(M_s, n, \epsilon)$. 


\begin{thebibliography}{99}
\bibitem{elf} Witkowski, Freeman  \emph{Incentive-Compatible Forecasting Competitions} \url{https://las.inf.ethz.ch/files/witkowski-etal-aaai2018.pdf}.

\end{thebibliography}

\end{document}
